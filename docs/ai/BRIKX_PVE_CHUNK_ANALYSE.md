# Brikx PvE-Check: Chunk-Pipeline Analyse
> Gegenereerd op 2026-02-23 â€” gebaseerd op volledige codebase-scan

---

## 1. Hoe chunks nu door Brikx gaan (volledig diagram)

```
GEBRUIKER
   â”‚
   â–¼
[1] UPLOAD  (/api/pve-check/upload)
   â”‚  extractDocument()
   â”‚  â†’ PDF/DOCX â†’ raw text (genormaliseerd)
   â”‚  â†’ sha256-hash, pageCount, wordCount
   â”‚  â†’ opgeslagen in Supabase Storage (pve-check-docs)
   â”‚  â†’ documentId terug naar frontend
   â”‚
   â–¼
[2] CLASSIFY  (lib/pveCheck/classify.ts)
   â”‚  buildNudge()  â† intake-context samengevat tot 1 string
   â”‚  llmClassify() â† gpt-4o-mini
   â”‚     â””â”€ input: text.slice(0, 12_000)  âš ï¸ HARDE TRUNCATIE
   â”‚     â””â”€ output: fields[] + missingFields[]
   â”‚  findEvidence() â† deterministisch keyword-match in pages[]
   â”‚  isVague()     â† deterministisch op vage-termen-lijst
   â”‚  â†’ PveClassifyResult {mappedData, fields, missingFieldContext}
   â”‚
   â–¼
[3] GAPS  (lib/pveCheck/gaps.ts)
   â”‚  Layer 1: structuralGaps()  â† rubric-items zonder classify-match
   â”‚  Layer 2: vagueGaps()       â† classify.fields met vague=true
   â”‚  Layer 3: contextGaps()     â† intake-gedreven regels (3 hardcoded)
   â”‚  â†’ PveGap[] gesorteerd op prioriteit (context > vague > structural)
   â”‚
   â–¼
[4] SCORE  (lib/pveCheck/score.ts)
   â”‚  Deterministisch: severity-weight Ã— status-factor
   â”‚  â†’ overallScore (0-100) + chapterScores[]
   â”‚
   â–¼
[5] KENNISBANK  (lib/pveCheck/knowledge.ts)
   â”‚  queryKnowledgeForGaps() â† parallel, 1 query per chapter
   â”‚  enrichGapsWithKnowledge() â† muteert gaps in-place (knowledgeHint)
   â”‚
   â–¼
[6] CONFLICTS  (lib/pveCheck/conflicts.ts)
   â”‚  6 hardcoded headline-conflicten
   â”‚  hergebruikt: computeBudgetWarning, computePermitStatus, scanRisks
   â”‚  â†’ HeadlineConflict[]
   â”‚
   â–¼
[7] PATCHES  (lib/pveCheck/patches.ts)
   â”‚  Alleen voor critical + important gaps
   â”‚  llmSuggestText() â† gpt-4o-mini per gap (sequentieel!)
   â”‚  buildPatchEvent() â† chapter + fieldPath + value
   â”‚  validatePatch()  â† CHAPTER_SCHEMAS validatie
   â”‚  â†’ PvePatchPlan[] {suggestedText, patchEvent?, validated}
   â”‚
   â–¼
[8] BENCHMARK  (lib/pveCheck/benchmark.ts)
   â”‚  computeBenchmark() â† statische benchmarks per projecttype
   â”‚  â†’ PveBenchmarkDelta[]
   â”‚
   â–¼
[9] DB INSERT  (Supabase: pve_check_results)
   â”‚  Opgeslagen: gaps, conflicts, mapped_data.patchEvents, scores
   â”‚  âš ï¸  BENCHMARK WORDT NIET OPGESLAGEN (lijn 252-257 route.ts)
   â”‚  âš ï¸  missingFieldContext (Map) verdwijnt hier â€” niet serialiseerbaar
   â”‚
   â–¼
[10] FRONTEND  (app/pve-check/components/ResultsPage.tsx)
   â”‚  Toont: score, chapterScores, conflicts, gaps gesorteerd op severity
   â”‚  patchMap â† mapped_data.patchEvents geÃ¯ndexeerd op delta.path
   â”‚
   â”‚  GRATIS VIEW:
   â”‚  â””â”€ gaps zichtbaar (label + reason + riskOverlay)
   â”‚  â””â”€ exampleGood (rubric) geblurd achter paywall
   â”‚
   â”‚  PREMIUM APPROVED VIEW:
   â”‚  â””â”€ verbetervoorstel (patchValue) zichtbaar
   â”‚  â””â”€ "Zet over naar PvE â†’" knop
   â”‚       â””â”€ onClick: alert(JSON.stringify(patch.delta))  âš ï¸ STUB!
   â”‚
   â–¼
EINDGEBRUIKER ZIET: score + lijst gaps + conflicts
           KRIJGT NIET: samenvatting, prioritering, actieplan, wizard-integratie
```

---

## 2. Waar gaat het fout â€” gedetailleerde knelpunten

### ðŸ”´ Knelpunt 1: Patch-â†’Wizard koppeling bestaat niet

**Locatie:** `ResultsPage.tsx` lijn 529

```typescript
onClick={() => alert("Patch: " + JSON.stringify(patch.delta))}
```

De "Zet over naar PvE â†’" knop is een `alert()` stub. Er is **geen router.push naar /wizard**, geen `useWizardStore.applyPatch()`, geen state-hydration. De patch-data is wel aanwezig, maar er ontbreekt de brug van PvE-check naar de wizard-state. Hetzelfde geldt voor de "Zet over naar Brikx PvE" knop in de premium action bar (lijn 150) â€” ook zonder `onClick`.

**Gevolg:** Alle gegenereerde `PvePatchPlan`s, inclusief de gevalideerde `PatchEvent`s, worden nooit daadwerkelijk toegepast op een bestaand PvE-document.

---

### ðŸ”´ Knelpunt 2: LLM ziet slechts 34% van het document

**Locatie:** `classify.ts` lijn 196

```typescript
content: `Beoordeel dit PvE document:\n\n${text.slice(0, 12000)}`
```

Het systeem accepteert tot 35.000 woorden (~175.000 tekens), maar stuurt slechts 12.000 tekens naar de LLM. Een gemiddeld PvE van 3.400 woorden (~20.000 tekens) wordt dus al deels afgekapt. Bij grotere documenten (verbouwing, nieuwbouw) mist de classificatie structureel de tweede helft â€” precies waar techniek, duurzaamheid en risico-paragrafen typisch staan.

**Gevolg:** Gaps worden aangemeld als "niet gevonden" terwijl de info wÃ©l in het document staat â€” maar na karakter 12.000.

---

### ðŸ”´ Knelpunt 3: Benchmark wordt niet opgeslagen Ã©n nooit getoond

**Locatie:** `route.ts` lijn 252-257 (compute) vs lijn 293-313 (insert)

De benchmark (`computeBenchmark`) wordt berekend maar is afwezig in de DB insert. Het resultaat (`PveBenchmarkDelta[]`) verdwijnt na de API-aanroep. In de `ResultsPage` is er geen benchmark-weergave.

**Gevolg:** Waardevolle context ("jouw PvE heeft 800 woorden, gemiddelde nieuwbouw-PvE heeft 3.400 woorden") wordt nooit getoond.

---

### ðŸŸ  Knelpunt 4: Geen aggregatie/advies-fase na gap-detectie

Er ontbreekt een **"chunk-summary agent"** die:
- De topgaps ordent op *gecombineerde impact* (severity Ã— fixEffort Ã— chapterScore)
- Een "Top 5 acties voor dit project" formuleert
- Kwantificeert: "50 gaps gevonden, 8 kritiek, focuseer eerst op: budget, techniek"
- Aanvullende vragen genereert voor info die ontbreekt (budgetbedrag, technische keuzes)

Momenteel wordt de ruwe `gaps[]` array direct naar de UI gestuurd, ongesorteerd op prioriteit voor dÃ­e specifieke klant.

---

### ðŸŸ  Knelpunt 5: missingFieldContext verdwijnt na classify

**Locatie:** `classify.ts` â†’ `route.ts` â†’ DB insert

`missingFieldContext` is een `Map<string, PveMissingFieldContext>` â€” JavaScript Maps zijn niet JSON-serialiseerbaar. In `gaps.ts` wordt de context correct gebruikt, maar na de gap-berekening gaat alle rijke "waarom ontbreekt dit / wat staat er wÃ©l in de buurt"-informatie verloren. Het wordt nooit opgeslagen en nooit aan de gebruiker getoond.

**Gevolg:** De gebruiker ziet een gap als "ontbreekt" maar krijgt niet de documentspecifieke uitleg die de LLM al had gegenereerd.

---

### ðŸŸ  Knelpunt 6: Patches worden sequentieel gegenereerd (performance)

**Locatie:** `patches.ts` lijn 145 â€” `for (const gap of patchableGaps)`

Elke `llmSuggestText()` is een afzonderlijke OpenAI API-call in een sequentiÃ«le loop. Bij 10 kritieke + belangrijke gaps = 10 seriÃ«le requests. Dit verlengt de analysetijd significant en is een bottleneck voor de UX.

---

### ðŸŸ¡ Knelpunt 7: contextGaps heeft slechts 3 hardcoded regels

**Locatie:** `gaps.ts` lijn 156-227

De intake-gedreven context-layer heeft maar 3 regels:
1. verbouwing zonder bouwjaar
2. ambitieuze duurzaamheid zonder energiedoel
3. hoog budget zonder risicoparagraaf

Er zijn geen regels voor: bijgebouw-specifieke eisen, postcode-gebaseerde context, archetype-specifieke gaps, of de combinatie van duurzaamheidsambitie met technische keuzes anders dan de energiedoel-check.

---

### ðŸŸ¡ Knelpunt 8: Aanvullings-loop bestaat niet

De `NeedsAdjustmentBanner` heeft een "Vul nu aan â†’" knop zonder implementatie. Er is geen mechanisme om:
- Te detecteren welke info ontbreekt die via follow-up vragen verkregen kan worden
- Follow-up vragen te genereren en naar de klant te sturen
- De analyse opnieuw te draaien na aanvulling
- Budget/techniek/duurzaamheid-specifieke vragen te stellen op basis van gap-patronen

---

## 3. Concrete refactor-punten (prioriteit)

### P1 â€” Wizard-patch-integratie (breekt de core loop)

```
ResultsPage: GapCard â†’ "Zet over naar PvE" knop
â†’ router.push('/wizard')
â†’ wizard-store.applyPatchEvents(result.mapped.patchEvents)
â†’ wizard opent bij het juiste chapter
```

Vereist: `useWizardStore.applyPatch(patchEvent: PatchEvent)` methode + routing-afspraak.

---

### P2 â€” Document-chunking voor LLM-classificatie

De 12.000-tekens truncatie moet vervangen worden door een sliding-window aanpak:

```
text.length > 12_000
  â†’ splits in overlappende chunks van ~8.000 tekens
  â†’ classifeer elk chunk parallel
  â†’ merge resultaten (hoogste confidence per fieldId wint)
```

Of: stuur samengevatte chunks per chapter door naar de LLM i.p.v. het ruwe document.

---

### P3 â€” ChunkSummaryAgent: aggregatie & actieplan

Voeg een nieuwe stap toe nÃ¡ de gap-detectie:

```typescript
// lib/pveCheck/summary.ts (nieuw)
export async function computeChunkSummary(
  gaps: PveGap[],
  conflicts: HeadlineConflict[],
  chapterScores: ChapterScore[],
  intake: PveCheckIntakeData
): Promise<ChunkSummary>

type ChunkSummary = {
  totalGapCount: number
  criticalCount: number
  weakestChapter: ChapterKey
  top5Actions: ActionItem[]      // gesorteerd op impact Ã— fixEffort
  followUpQuestions: string[]   // vragen voor ontbrekende info
  oneLineSummary: string        // "Jouw PvE scoort 54/100 â€” focus op budget en techniek"
}
```

Dit lost het "check-output armoede" probleem op en geeft de gebruiker direct bruikbaar actieadvies.

---

### P4 â€” Benchmark opslaan + tonen

In `route.ts`: voeg `benchmark_data: benchmark` toe aan de DB-insert.
In `ResultsPage`: voeg een `BenchmarkCard` toe:

```
"Jouw PvE: 800 woorden â€” gemiddelde verbouwing: 2.200 woorden
 Jouw PvE is dunner dan gebruikelijk. Voeg ruimteprogramma en techniek toe."
```

---

### P5 â€” missingFieldContext serialiseren

`missingFieldContext` van type `Map` moet worden omgezet naar een plain object voor opslag:

```typescript
// In classify.ts - return als object i.p.v. Map
missingFieldContext: Object.fromEntries(missingFieldContext)
// In gaps.ts - gebruik als Record<string, ...>
// In DB: sla op als JSON-kolom
// In ResultsPage: toon "waarom ontbreekt dit" per gap
```

---

### P6 â€” Patches paralleliseren

```typescript
// patches.ts
const plans = await Promise.all(
  patchableGaps.map((gap) => generatePatchPlan(gap, documentText, knowledgeMap))
)
```

---

### P7 â€” relevantChapters-prioritering voor check-output

Voeg een `relevantChapters` berekening toe die per intake-archetype bepaalt welke hoofdstukken het zwaarst wegen â€” zodat de check-output prioritering projecttype-bewust is:

```typescript
// Nieuwbouw: basis + ruimtes + techniek wegen zwaarder
// Verbouwing: basis + techniek + risico wegen zwaarder
// Bijgebouw: ruimtes + vergunning wegen zwaarder
```

---

### P8 â€” Aanvullings-loop implementeren

Na gap-analyse: als â‰¥3 critical gaps betrekking hebben op ontbrekende informatie (geen vagueness, geen conflict):

```
1. Genereer 2-3 follow-up vragen (LLM of templates per gap-type)
2. Sla op als pending_questions[] in pve_check_results
3. Email klant / toon in "Vul nu aan" flow
4. Na beantwoording: heranalyse met aangevulde intake-context
```

---

## 4. Samenvatting: de fundamentele gap

```
WAT BRIKX DOET:
  Document â†’ Chunks â†’ Supabase (rijke data opgeslagen) âœ“

WAT ONTBREEKT:
  Supabase data â†’ Actiegericht advies â†’ Wizard â†’ Betere PvE
                       â†‘
               HIER ZIT DE GAP

De pipeline produceert uitstekende intermediaire data
(gaps met evidence, patches met suggesties, conflicts met next steps)
maar geen van deze data vertaalt zich naar:
  - Een concreet actieplan voor de gebruiker
  - Automatische verbetering van de PvE via de wizard
  - Follow-up vragen voor ontbrekende info
```

De drie dringendste fixes:
1. **P1**: Wizard-patch-integratie â€” de "Zet over naar PvE" knop moet werken
2. **P2**: Document-chunking â€” de LLM moet het hele document zien
3. **P3**: ChunkSummaryAgent â€” aggregeer gaps naar een top-5 actieplan
